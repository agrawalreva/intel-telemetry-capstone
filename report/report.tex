% !TEX TS-program = xelatex
% !BIB TS-program = bibtex
\documentclass[12pt,letterpaper]{article}
\usepackage{style/dsc180reportstyle} % import dsc180reportstyle.sty
\usepackage{graphicx} % for including images

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Title and Authors
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Balancing Privacy and Utility: Differentially Private Synthetic Telemetry Data Generation}

\author{
\textbf{Reva Agrawal} \\
{\tt ragrawal@ucsd.edu}
\and
\textbf{Jordan Lambino} \\
{\tt jlambino@ucsd.edu}
\and
\textbf{Dhruv Patel} \\
{\tt dhp005@ucsd.edu}
\and
\textbf{Yu-Xiang Wang} \\
{\tt yuxiangw@ucsd.edu}
\and
\textbf{Bijan Arbab} \\
{\tt barbab@ucsd.edu}
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Abstract and Links
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
This research investigates the feasibility and accuracy of generating synthetic datasets that preserve privacy while maintaining statistical utility, using Intel telemetry data as a case study. The data encompasses device- and user-level event logs used for product health, diagnostics, and quality monitoring. Our primary goal is to explore how well differentially private mechanisms, including those inspired by the Private Evolution line of work, can reproduce key analytical quantities without compromising user-level privacy. By comparing results on the original data with those on synthetic or DP-noisy data using metrics aligned to prior analytical queries, we aim to evaluate the trade-off between privacy protection and data utility. The results will help determine whether differential privacy can support the release of realistic yet privacy-preserving telemetry data for product analytics and quality monitoring.
\end{abstract}

\begin{center}
Code: \url{https://github.com/agrawalreva/intel-telemetry-capstone}

Website: \url{https://agrawalreva.github.io/intel-telemetry-capstone/}
\end{center}

\maketoc
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Main Contents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

Privacy concerns have become increasingly critical in data-driven software environments, where telemetry data often includes highly sensitive user-level event logs that can reveal individual usage patterns, product preferences, and behavioral characteristics. Even when telemetry datasets are anonymized, there remains a significant risk of re-identification through auxiliary information or linkage attacks. To address this challenge, our research explores how differential privacy (DP) can be used to generate synthetic datasets that protect individual user records while maintaining the analytical value of the underlying data for product quality monitoring and error rate analysis.

We focus on Intel telemetry data, which supports product health, diagnostics, and quality monitoring through device-level and user-level event logs. Our analytical setting uses a specialized schema derived from queries that Intel has employed in prior studies; the goal is to compute and preserve key metrics and query outcomes that are essential for identifying problematic products and prioritizing quality improvements.

We apply both baseline differentially private mechanisms (such as adding calibrated noise to counts) and advanced synthetic data generation techniques to produce privacy-preserving versions of the dataset. We then evaluate how closely the synthetic data aligns with the original in terms of statistical relationships, error rate distributions, and the ability to identify products with anomalously high error rates through z-score analysis. This evaluation provides a foundation for future privacy-preserving data sharing in software analytics and product quality monitoring.

\subsection{Literature Review}
Previous researchers have explored differential privacy for various data types, with particular attention to text data and structured datasets. As mentioned in the context of text data, differential privacy is paramount particularly in domains where individuals' information can be recovered after model training. Part of this problem stems from the fact that some applications are trained using private data, which could be completely recalled or leaked by the model. A popular method in response to this issue involves fine-tuning language models and using DP-SGD (Differential Privacy - Stochastic Gradient Descent) for synthetic text generation. Despite the proven guarantee of differential privacy, the DP-SGD fine-tuning approach is computationally demanding and therefore lacks scalability.

The paper in question proposes a method of ensuring privacy by creating a synthetic dataset which retains important aggregate information while protecting the data of any one individual. To combat this issue of excessive resource use, the authors propose a method in \cite{xie2024differentially} which utilizes model API access in order to generate differentially private synthetic text data, rather than relying on access to the model weights.

The authors leverage an established method, Private Evolution, previously used to generate DP synthetic images. However, this approach cannot be applied directly to text data due to variable lengths. The authors suggest AUG-PE (Augmented Private Evolution) as an alternative to work with text data, accounting for increased complexity. The AUG-PE algorithm relies on three primary elements:

\begin{enumerate}
    \item RANDOM\_API: This component uses the model API to produce random samples from prompts. In this step, each prompt includes a particular class label and/or subcategories (named "pseudo-classes") for increased diversity. For example, prompts about "movies" may include subcategories such as "horror films" or "documentaries" to generate a diverse, initial set of candidates.
    \item DP\_NN\_HISTOGRAM: The purpose of this step is to privately select samples for post-processing. In essence, this process functions by conducting "nearest neighbor" votes for each sample, and creating a histogram based on those votes. This step guarantees differential privacy by adding Gaussian noise to those vote counts, prior to histogram generation.
    \item VARIATION\_API: This step adds an additional element of randomness to the synthetic samples; in other words, this part of the algorithm generates "variations" of the input samples. The authors input "paraphrasing" and "fill-in-the-blanks" prompts into the model for this component of AUG-PE. Through this process, the algorithm adds variance to the text samples while retaining important characteristics.
\end{enumerate}

The authors apply the AUG-PE algorithm to Yelp Review, OpenReview, and PubMed datasets. For text generation capabilities, the authors primarily leverage GPT-2, GPT-2-Medium, GPT-2-Large, and GPT-3.5, among several other models. As part of the model selection process, the authors test on a combination of open-source and closed-source models, displaying the flexibility of API-access-only approaches. As a result of their experiments, the authors found that AUG-PE generates high-utility, differentially private text data. On top of the differential privacy guarantee, AUG-PE, in comparison to DP fine-tuning approaches which require model weight access, showed to be much more efficient. The authors' experiments demonstrate the power of leveraging LLMs for DP synthetic text generation, revealing that API-access provides a much more scalable solution in comparison to known "state-of-the-art" approaches.

For structured event log data like telemetry, similar principles apply: we need mechanisms that can preserve aggregate statistics (such as error rates and event counts) while protecting individual user records. The Gaussian mechanism and Laplace mechanism are commonly used for count queries and statistical summaries, which are central to our telemetry analysis.

\subsection{Dataset Description}

Our data is drawn from Intel telemetry accessed through Globus. The source data consists of 23 tables spread across two schemas: \textbf{university\_analysis\_pad} and \textbf{university\_prod}. A specialized schema was derived from a consolidated set of analytical queries used in prior Intel studies; the exploration of these queries catalogs 24 queries spanning battery usage, display devices, browser and web category usage, MODS sleep and blockers, power and memory utilization, and process-level analytics. We use a subset of 12 of these queries as our benchmark set. The specialized schema restricts the working corpus to the reporting tables required to execute those queries, making large-scale experimentation tractable.

The reporting layer built on top of the raw tables includes pre-aggregated tables such as \textbf{system\_batt\_dc\_events} (battery power-on counts and duration per device); the normalized sysinfo table \textbf{system\_sysinfo\_unique\_normalized} (country, chassis type, model, OS, RAM, persona, and related system attributes); \textbf{system\_cpu\_metadata} (market codename and CPU generation); and \textbf{system\_display\_devices} (connection type, resolution, vendor, and AC/DC duration). Further tables include \textbf{system\_mods\_top\_blocker\_hist} and \textbf{system\_os\_codename\_history} (MODS blockers by OS name and codename), \textbf{system\_web\_cat\_usage} and \textbf{system\_web\_cat\_pivot\_duration} (browser and persona-level web category usage), \textbf{system\_on\_off\_suspend\_time\_day} (on/off and MODS sleep summary), \textbf{system\_hw\_pkg\_power} (package power by device), \textbf{system\_memory\_utilization} (RAM utilization histograms), and \textbf{system\_mods\_power\_consumption} (process-level power consumption for rankings). Records are keyed by device identifier \textbf{guid}. This structure supports the 12 benchmark queries that we use to establish a privacy-free baseline and to evaluate differentially private mechanisms. We develop and validate our pipeline on a subsample database before running on the full specialized dataset.

Differential privacy is especially relevant for telemetry data because events are often tied to devices or users over time. Even after removing direct identifiers, sequences of events can support re-identification or inference of sensitive behavior. Applying DP to telemetry, whether via noisy query answers or via synthetic data generation, provides a formal guarantee that the output does not depend too strongly on any single individual's or device's contribution, which helps protect against reconstruction and linkage attacks while still allowing aggregate and distributional analysis.

\section{Methods}

\subsection{Data Access and Database Construction}
\label{sec:data_access}

Our pipeline begins with raw Intel telemetry data accessed through Globus. The source data consists of 23 tables spread across two schemas: \texttt{university\_analysis\_pad} and \texttt{university\_prod}. To support efficient development and later production runs, we built two DuckDB databases. A subsample database (approximately 5\,GB total, with each table capped at 200\,MB) allows rapid iteration and validation of the pipeline. A full database (approximately 3.6\,TB) is reserved for final production runs. Both are created from the same source tables using database creation scripts so that query logic and downstream mechanisms can be developed on the subsample and then run unchanged on the full dataset.

\subsection{Reporting Layer and Baseline Queries}
\label{sec:reporting}

On top of the raw tables we constructed a reporting layer of 22 pre-aggregated tables using a SQL build script (\texttt{00\_build\_reporting\_tables.sql}). This script resolves joins across schemas, normalizes column names, and structures the data for analytics. From this reporting layer we wrote 12 benchmark queries covering a range of analytical use cases: battery usage by geography and CPU family, display device vendor market share, browser popularity by country, OS-level MODS blockers, RAM utilization distributions, persona-level web category usage, and process power rankings. The results of these 12 queries were exported to CSV files and serve as our privacy-free baseline for evaluating the impact of differential privacy.

\subsection{Differential Privacy Mechanisms}
\label{sec:methods}

We implemented two differentially private mechanisms by hand, without relying on external DP libraries. The first is a Gaussian mechanism providing $(\epsilon, \delta)$-differential privacy; the second is a Laplace mechanism providing pure $\epsilon$-differential privacy. Both follow user-level privacy semantics: the sensitivity of each query is bounded under the assumption that adding or removing one device (one \texttt{guid}) can affect the output by at most a known amount. For each mechanism we loop over 11 epsilon values (from 0.01 to $\infty$) using a fixed random seed for reproducibility. Calibrated noise is added to the numeric columns of each query result. Post-processing is applied to ensure interpretability: negative values are clamped to zero where appropriate, and percentage columns are re-normalised so that they sum to a valid distribution. This design keeps the privacy guarantee intact while producing outputs that are directly comparable to the baseline CSVs.

\subsection{Evaluation Metrics}
\label{sec:eval_metrics}

Each of the 12 queries is evaluated using the metric that best matches its analytical intent. For queries that aim to identify anomalous groups (Q1, Q2, Q5, Q7, Q9), we use z-score preservation together with Intersection over Union (IOU) of the identified anomaly sets. For queries that output percentage or count distributions (Q4, Q10, Q11), we use Total Variation Distance between the baseline and DP distributions. For ranking-style queries (Q3, Q12), we use Kendall's $\tau$ to measure agreement between the baseline and DP rankings. For winner-per-group queries (Q6), we use Top-1 Accuracy. For queries that produce multi-dimensional distributions (Q8), we use KL Divergence. This query-specific choice of metrics ensures that utility is measured in terms that reflect how each result would be used in practice. Evaluation scripts will select the best epsilon per query (or per mechanism) and generate privacy-utility tradeoff visualisations; the pipeline has been tested on the subsample database, with the full-dataset run planned as the final step.

\section{Results}

\subsection{Subsample Pipeline}

% (To be filled.)

\subsection{Full Schema Results}

% (To be filled.)

\section{Discussion}

% (To be filled.)

\section{Conclusion}

\subsection{Key Findings}

% (To be filled.)

\subsection{Contributions}

\begin{itemize}
    \item \textbf{Reva Agrawal}: Carried out the exploration of the 22 SQL queries to identify the tables and columns required to build the specialized schema. Confirmed and refined the pipeline and evaluation metrics with the project advisor and debugged its implementation. Wrote section 1 and 2 of the report based on progressive work over the course of the project.
    \item \textbf{Dhruv Patel}: Responsible for the design and construction of the database and pipeline, including setting up DuckDB from the raw data, building the reporting layer in SQL, and writing the scripts to export the baseline. Implemented both differential privacy mechanisms. Designed the evaluation metrics in line with the project mentor's feedback. Ensured that no sensitive data was committed to the repository.
    \item \textbf{Jordan Lambino}: Developed code for the project website, added to website design, and included information in the Introduction and Methods sections. Assisted with query exploration along with group members. Wrote parts of the Introduction section for the report and reviewed report for accuracy. 

\end{itemize}

\subsection{Future Work}

% (To be filled.)

\subsection{Final Remarks}

% (To be filled.)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Reference / Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\makereference
\nocite{*}

\bibliography{reference}
\bibliographystyle{style/dsc180bibstyle}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\makeappendix

\subsection{Schema Exploration}
\url{https://docs.google.com/document/d/1XfkwK9zN7aP9EJ5fI-bRJ5eS9TzQZW_oRQ9NwRilQeo/edit?usp=sharing}

\subsection{Project Proposal}
\url{https://drive.google.com/file/d/1-uWLW_FLOvIJ-335J9SwYmFleC-lXZaR/view?usp=sharing}

\subsection{Additional Results}
% (To be filled.)

\subsection{Training Details}
% (To be filled.)

\subsection{Additional Figures}
% (To be filled.)


\end{document}
